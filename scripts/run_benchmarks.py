from __future__ import annotations

import json
import os
import statistics
import time
from dataclasses import dataclass
from pathlib import Path

import psycopg
from dotenv import load_dotenv

load_dotenv()

ROOT = Path(__file__).resolve().parents[1]
SCENARIOS_DIR = ROOT / "sql_lab" / "scenarios"
REPORTS_DIR = ROOT / "sql_lab" / "reports"
README_PATH = ROOT / "README.md"
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://lab:lab@localhost:5433/sql_lab")
RUNS = int(os.getenv("BENCH_RUNS", "12"))
WARMUPS = int(os.getenv("BENCH_WARMUPS", "3"))
SHOWCASE_START = "<!-- BENCHMARK_SHOWCASE:START -->"
SHOWCASE_END = "<!-- BENCHMARK_SHOWCASE:END -->"


@dataclass
class BenchResult:
    name: str
    mode: str
    p50_ms: float
    p95_ms: float
    avg_ms: float
    explain: dict


def format_markdown_report(report: dict) -> str:
    scenario = report["scenario"]
    before = report["before"]
    after = report["after"]
    improvement = report["improvement_percent"]

    direction = "faster" if improvement >= 0 else "slower"
    abs_improvement = abs(improvement)

    return (
        f"# Benchmark Report: `{scenario}`\n\n"
        f"- Runs: `{report['runs']}`\n"
        f"- Warmups: `{report['warmups']}`\n"
        f"- Result: **{abs_improvement}% {direction}**\n\n"
        "## Latency (ms)\n\n"
        "| Metric | Before | After |\n"
        "|---|---:|---:|\n"
        f"| p50 | {before['p50_ms']} | {after['p50_ms']} |\n"
        f"| p95 | {before['p95_ms']} | {after['p95_ms']} |\n"
        f"| avg | {before['avg_ms']} | {after['avg_ms']} |\n\n"
        "## Notes\n\n"
        "- Full execution plan is stored in JSON report (`before.explain`, `after.explain`).\n"
        "- Compare scan nodes (`Seq Scan` vs `Index Scan`) and sort/hash costs in the plan.\n"
    )


def write_summary_markdown(items: list[tuple[str, float, float, float, str]]) -> None:
    lines = [
        "# SQL Performance Lab Summary",
        "",
        "| Scenario | Avg Before (ms) | Avg After (ms) | Improvement (%) |",
        "|---|---:|---:|---:|",
    ]
    for name, before_avg, after_avg, improvement, _ in items:
        lines.append(f"| `{name}` | {before_avg} | {after_avg} | {improvement} |")
    lines.append("")
    lines.append("Generated by `scripts/run_benchmarks.py`.")
    (REPORTS_DIR / "summary.md").write_text("\n".join(lines) + "\n")


def build_showcase_markdown(items: list[tuple[str, float, float, float, str]], top_n: int = 3) -> str:
    ranked = sorted(items, key=lambda x: x[3], reverse=True)
    top = ranked[:top_n]

    lines = [
        "### Live Benchmark Showcase",
        "",
        "_Этот блок обновляется автоматически после `make run`._",
        "",
        "| Scenario | Avg Before (ms) | Avg After (ms) | Improvement (%) | Report |",
        "|---|---:|---:|---:|---|",
    ]
    for name, before_avg, after_avg, improvement, _ in top:
        report_link = f"`sql_lab/reports/{name}.report.md`"
        lines.append(f"| `{name}` | {before_avg} | {after_avg} | {improvement} | {report_link} |")
    lines.append("")
    return "\n".join(lines)


def update_readme_showcase(items: list[tuple[str, float, float, float, str]]) -> None:
    if not README_PATH.exists():
        return

    content = README_PATH.read_text()
    start_idx = content.find(SHOWCASE_START)
    end_idx = content.find(SHOWCASE_END)
    if start_idx == -1 or end_idx == -1 or start_idx >= end_idx:
        return

    body = build_showcase_markdown(items)
    replace_from = start_idx + len(SHOWCASE_START)
    new_content = content[:replace_from] + "\n" + body + "\n" + content[end_idx:]
    README_PATH.write_text(new_content)


def split_sql(sql_text: str) -> tuple[list[str], str]:
    # Для формата лаборатории считаем, что последний statement - это
    # benchmark query, а все предыдущие - setup (например CREATE INDEX).
    statements = [s.strip() for s in sql_text.split(";") if s.strip()]
    if not statements:
        raise ValueError("Empty SQL")
    if len(statements) == 1:
        return [], statements[0]
    return statements[:-1], statements[-1]


def explain_json(cur: psycopg.Cursor, query: str) -> dict:
    cur.execute(f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}")
    return cur.fetchone()[0][0]


def run_query(cur: psycopg.Cursor, query: str) -> float:
    started = time.perf_counter()
    cur.execute(query)
    cur.fetchall()
    elapsed = (time.perf_counter() - started) * 1000
    return elapsed


def run_single(cur: psycopg.Cursor, name: str, mode: str, sql_path: Path) -> BenchResult:
    setup_stmts, main_query = split_sql(sql_path.read_text())

    for stmt in setup_stmts:
        cur.execute(stmt)

    # Обновляем статистику перед прогонами, чтобы план был ближе к реальности.
    cur.execute("ANALYZE users;")
    cur.execute("ANALYZE orders;")

    # Warmup выравнивает "первый холодный запуск", чтобы не искажать метрики.
    for _ in range(WARMUPS):
        run_query(cur, main_query)

    timings = [run_query(cur, main_query) for _ in range(RUNS)]
    timings.sort()
    p50 = timings[len(timings) // 2]
    p95 = timings[min(len(timings) - 1, int(len(timings) * 0.95))]
    avg = statistics.fmean(timings)
    plan = explain_json(cur, main_query)

    return BenchResult(name=name, mode=mode, p50_ms=round(p50, 2), p95_ms=round(p95, 2), avg_ms=round(avg, 2), explain=plan)


def collect_scenarios() -> list[str]:
    names = set()
    for p in SCENARIOS_DIR.glob("*.before.sql"):
        names.add(p.name.replace(".before.sql", ""))
    return sorted(names)


def main() -> None:
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    scenarios = collect_scenarios()
    if not scenarios:
        raise RuntimeError("No scenarios found")

    summary = []

    with psycopg.connect(DATABASE_URL, autocommit=True) as conn:
        with conn.cursor() as cur:
            for name in scenarios:
                before_path = SCENARIOS_DIR / f"{name}.before.sql"
                after_path = SCENARIOS_DIR / f"{name}.after.sql"
                if not after_path.exists():
                    raise RuntimeError(f"Missing pair for scenario: {name}")

                before = run_single(cur, name, "before", before_path)
                after = run_single(cur, name, "after", after_path)

                improvement = round(((before.avg_ms - after.avg_ms) / before.avg_ms) * 100, 2) if before.avg_ms > 0 else 0.0

                report = {
                    "scenario": name,
                    "runs": RUNS,
                    "warmups": WARMUPS,
                    "before": before.__dict__,
                    "after": after.__dict__,
                    "improvement_percent": improvement,
                }

                report_path = REPORTS_DIR / f"{name}.report.json"
                report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))

                md_path = REPORTS_DIR / f"{name}.report.md"
                md_path.write_text(format_markdown_report(report))
                summary.append((name, before.avg_ms, after.avg_ms, improvement, str(report_path)))

    write_summary_markdown(summary)
    update_readme_showcase(summary)

    print("Benchmark results:")
    for name, b, a, imp, rp in summary:
        print(f"- {name}: avg {b} ms -> {a} ms ({imp}%), report={rp}")


if __name__ == "__main__":
    main()
